{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추천 시스템 제작(하루3끼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %autosave 0\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "from tensorflow.keras.models import Sequential  # class\n",
    "from tensorflow.keras.models import load_model  # model 사용\n",
    "from tensorflow.keras.layers import Dense       # 전결합\n",
    "from tensorflow.keras.layers import Dropout     # 특정 node를 사용안함.\n",
    "from tensorflow.keras.callbacks import EarlyStopping   # 학습 자동 중지\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint # 우수한 학습 모델 파일 저장\n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras.utils import to_categorical   # one-hot 엔코딩\n",
    "from tensorflow.keras.optimizers import Adam    # 가중치, bias 최적화\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split # 학습셋과 테스트셋의 분리 지원\n",
    "from sklearn.model_selection import StratifiedKFold  # K겹 교차 검증\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "import platform \n",
    "\n",
    "if (platform.system() == 'Windows'):  # Windows, Linux, Darwin\n",
    "    rc('font', family=font_manager.FontProperties(fname=\"C:/Windows/Fonts/malgun.ttf\").get_name())\n",
    "    path = '.' # Local\n",
    "else:    \n",
    "    plt.rc('font', family='NanumBarunGothic')  # Ubuntu 18.04 기준 한글 처리\n",
    "    path = '/content/drive/My Drive/ai7/dnn/recommendation' # Colab\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12         # 글자 크기\n",
    "# plt.rcParams[\"figure.figsize\"] = (10, 4) # 10:4의 그래프 비율\n",
    "plt.rcParams['axes.unicode_minus'] = False  # minus 부호는 unicode 적용시 한글이 깨짐으로 설정\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(21771, 26)\n"
     ]
    }
   ],
   "source": [
    "# header가 있을경우 skiprows=1 선언\n",
    "data = np.loadtxt(path + '/train1.csv', delimiter=',', skiprows=1, dtype=np.float64)   # 특성이 작은 데이터의 예외 추가\n",
    "print(type(data))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21771, 25)\n",
      "(21771,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터와 class의 분리\n",
    "# 0: 닭가슴살\n",
    "# 1: 간편요리\n",
    "# 2: 샐러드\n",
    "# 3: 건강미용\n",
    "# 4: 간식\n",
    "X = data[:, 0:25]  # 0 ~ 24\n",
    "print(X.shape)\n",
    "Y = data[:, 25]    # 25 번째 데이터, class의 분리\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. ... 4. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "Y = Y.astype('int') # 정수로 형변환\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 0: 닭가슴살\n",
    "# 1: 간편요리\n",
    "# 2: 샐러드\n",
    "# 3: 건강미용\n",
    "# 4: 간식\n",
    "Y_encoded = to_categorical(Y) # one-hot-encoding\n",
    "\n",
    "print(Y_encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96 0.01 0.01 0.01 0.01 0.96 0.01 0.01 0.01 0.01 0.96 0.01 0.01 0.01\n",
      " 0.01 0.96 0.01 0.01 0.01 0.01 0.96 0.01 0.01 0.01 0.01]\n",
      "[1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(Y_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "(3919, 5)\n"
     ]
    }
   ],
   "source": [
    "# train_test_split 분할을 통한 훈련, 검증, 테스트 데이터의 분리\n",
    "seed = 0\n",
    "# 90%: 분할대기, 10%: 테스트\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(X, Y_encoded,\n",
    "                                           stratify=Y_encoded,\n",
    "                                           test_size=0.1,\n",
    "                                           random_state=seed)\n",
    "# 나머지 데이터 90%를 분할, 80%: 훈련, 20%: 검증\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all,\n",
    "                                           stratify=y_train_all,\n",
    "                                           test_size=0.2,\n",
    "                                           random_state=seed)\n",
    "\n",
    "print(y_val)\n",
    "print(y_val.shape)\n",
    "# 닭가슴살: 0 , 간편요리: 1, 샐러드: 2 , 건강미용 : 3 , 간식 : 4\n",
    "# (67, 5): 67건의 데이터가 입력되어 한건당 5가지에 속할 확률이 출력됨으로 67행 5열이됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "(2178, 5)\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(y_test.shape)\n",
    "# (37, 5): 37건의 데이터가 입력되어 한건당 5가지에 속할 확률이 출력됨으로 37행 5열이됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 5)                 130       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160\n",
      "Trainable params: 160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3123/3135 [============================>.] - ETA: 0s - loss: 1.0104 - accuracy: 0.6052\n",
      "Epoch 1: val_accuracy improved from -inf to 0.71651, saving model to .\\3kki6.h5\n",
      "3135/3135 [==============================] - 10s 3ms/step - loss: 1.0091 - accuracy: 0.6054 - val_loss: 0.7064 - val_accuracy: 0.7165\n",
      "Epoch 2/100\n",
      "3120/3135 [============================>.] - ETA: 0s - loss: 0.6291 - accuracy: 0.7286\n",
      "Epoch 2: val_accuracy improved from 0.71651 to 0.74228, saving model to .\\3kki6.h5\n",
      "3135/3135 [==============================] - 8s 3ms/step - loss: 0.6288 - accuracy: 0.7287 - val_loss: 0.5739 - val_accuracy: 0.7423\n",
      "Epoch 3/100\n",
      "3116/3135 [============================>.] - ETA: 0s - loss: 0.5511 - accuracy: 0.7433\n",
      "Epoch 3: val_accuracy improved from 0.74228 to 0.75478, saving model to .\\3kki6.h5\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.5510 - accuracy: 0.7433 - val_loss: 0.5293 - val_accuracy: 0.7548\n",
      "Epoch 4/100\n",
      "3104/3135 [============================>.] - ETA: 0s - loss: 0.5201 - accuracy: 0.7494\n",
      "Epoch 4: val_accuracy did not improve from 0.75478\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.5202 - accuracy: 0.7495 - val_loss: 0.5096 - val_accuracy: 0.7543\n",
      "Epoch 5/100\n",
      "3135/3135 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.7511\n",
      "Epoch 5: val_accuracy improved from 0.75478 to 0.75504, saving model to .\\3kki6.h5\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.5056 - accuracy: 0.7511 - val_loss: 0.4998 - val_accuracy: 0.7550\n",
      "Epoch 6/100\n",
      "3120/3135 [============================>.] - ETA: 0s - loss: 0.4978 - accuracy: 0.7516\n",
      "Epoch 6: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4977 - accuracy: 0.7517 - val_loss: 0.4942 - val_accuracy: 0.7522\n",
      "Epoch 7/100\n",
      "3123/3135 [============================>.] - ETA: 0s - loss: 0.4930 - accuracy: 0.7516\n",
      "Epoch 7: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4928 - accuracy: 0.7518 - val_loss: 0.4906 - val_accuracy: 0.7522\n",
      "Epoch 8/100\n",
      "3131/3135 [============================>.] - ETA: 0s - loss: 0.4899 - accuracy: 0.7536\n",
      "Epoch 8: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4896 - accuracy: 0.7537 - val_loss: 0.4880 - val_accuracy: 0.7489\n",
      "Epoch 9/100\n",
      "3117/3135 [============================>.] - ETA: 0s - loss: 0.4872 - accuracy: 0.7537\n",
      "Epoch 9: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4872 - accuracy: 0.7536 - val_loss: 0.4862 - val_accuracy: 0.7499\n",
      "Epoch 10/100\n",
      "3125/3135 [============================>.] - ETA: 0s - loss: 0.4857 - accuracy: 0.7538\n",
      "Epoch 10: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4854 - accuracy: 0.7539 - val_loss: 0.4847 - val_accuracy: 0.7504\n",
      "Epoch 11/100\n",
      "3123/3135 [============================>.] - ETA: 0s - loss: 0.4842 - accuracy: 0.7540\n",
      "Epoch 11: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4840 - accuracy: 0.7541 - val_loss: 0.4835 - val_accuracy: 0.7504\n",
      "Epoch 12/100\n",
      "3112/3135 [============================>.] - ETA: 0s - loss: 0.4827 - accuracy: 0.7541\n",
      "Epoch 12: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 8s 3ms/step - loss: 0.4829 - accuracy: 0.7539 - val_loss: 0.4826 - val_accuracy: 0.7489\n",
      "Epoch 13/100\n",
      "3106/3135 [============================>.] - ETA: 0s - loss: 0.4817 - accuracy: 0.7542\n",
      "Epoch 13: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 8s 2ms/step - loss: 0.4820 - accuracy: 0.7540 - val_loss: 0.4818 - val_accuracy: 0.7489\n",
      "Epoch 14/100\n",
      "3128/3135 [============================>.] - ETA: 0s - loss: 0.4814 - accuracy: 0.7540\n",
      "Epoch 14: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4813 - accuracy: 0.7540 - val_loss: 0.4811 - val_accuracy: 0.7489\n",
      "Epoch 15/100\n",
      "3120/3135 [============================>.] - ETA: 0s - loss: 0.4807 - accuracy: 0.7541\n",
      "Epoch 15: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4806 - accuracy: 0.7541 - val_loss: 0.4807 - val_accuracy: 0.7484\n",
      "Epoch 16/100\n",
      "3125/3135 [============================>.] - ETA: 0s - loss: 0.4804 - accuracy: 0.7536\n",
      "Epoch 16: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4801 - accuracy: 0.7537 - val_loss: 0.4802 - val_accuracy: 0.7479\n",
      "Epoch 17/100\n",
      "3132/3135 [============================>.] - ETA: 0s - loss: 0.4799 - accuracy: 0.7533\n",
      "Epoch 17: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4796 - accuracy: 0.7535 - val_loss: 0.4798 - val_accuracy: 0.7482\n",
      "Epoch 18/100\n",
      "3127/3135 [============================>.] - ETA: 0s - loss: 0.4794 - accuracy: 0.7532\n",
      "Epoch 18: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4792 - accuracy: 0.7533 - val_loss: 0.4794 - val_accuracy: 0.7494\n",
      "Epoch 19/100\n",
      "3121/3135 [============================>.] - ETA: 0s - loss: 0.4789 - accuracy: 0.7538\n",
      "Epoch 19: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4789 - accuracy: 0.7538 - val_loss: 0.4792 - val_accuracy: 0.7499\n",
      "Epoch 20/100\n",
      "3122/3135 [============================>.] - ETA: 0s - loss: 0.4787 - accuracy: 0.7541\n",
      "Epoch 20: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4786 - accuracy: 0.7542 - val_loss: 0.4789 - val_accuracy: 0.7499\n",
      "Epoch 21/100\n",
      "3131/3135 [============================>.] - ETA: 0s - loss: 0.4786 - accuracy: 0.7539\n",
      "Epoch 21: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 8s 3ms/step - loss: 0.4783 - accuracy: 0.7541 - val_loss: 0.4786 - val_accuracy: 0.7510\n",
      "Epoch 22/100\n",
      "3125/3135 [============================>.] - ETA: 0s - loss: 0.4784 - accuracy: 0.7540\n",
      "Epoch 22: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 10s 3ms/step - loss: 0.4781 - accuracy: 0.7541 - val_loss: 0.4784 - val_accuracy: 0.7510\n",
      "Epoch 23/100\n",
      "3127/3135 [============================>.] - ETA: 0s - loss: 0.4780 - accuracy: 0.7540\n",
      "Epoch 23: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 12s 4ms/step - loss: 0.4778 - accuracy: 0.7541 - val_loss: 0.4782 - val_accuracy: 0.7517\n",
      "Epoch 24/100\n",
      "3126/3135 [============================>.] - ETA: 0s - loss: 0.4778 - accuracy: 0.7540\n",
      "Epoch 24: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 11s 4ms/step - loss: 0.4776 - accuracy: 0.7541 - val_loss: 0.4780 - val_accuracy: 0.7517\n",
      "Epoch 25/100\n",
      "3124/3135 [============================>.] - ETA: 0s - loss: 0.4778 - accuracy: 0.7544\n",
      "Epoch 25: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 11s 3ms/step - loss: 0.4775 - accuracy: 0.7545 - val_loss: 0.4779 - val_accuracy: 0.7517\n",
      "Epoch 26/100\n",
      "3121/3135 [============================>.] - ETA: 0s - loss: 0.4773 - accuracy: 0.7554\n",
      "Epoch 26: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 8s 3ms/step - loss: 0.4773 - accuracy: 0.7553 - val_loss: 0.4777 - val_accuracy: 0.7517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "3130/3135 [============================>.] - ETA: 0s - loss: 0.4772 - accuracy: 0.7547\n",
      "Epoch 27: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 8s 3ms/step - loss: 0.4771 - accuracy: 0.7547 - val_loss: 0.4775 - val_accuracy: 0.7522\n",
      "Epoch 28/100\n",
      "3135/3135 [==============================] - ETA: 0s - loss: 0.4770 - accuracy: 0.7544\n",
      "Epoch 28: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4770 - accuracy: 0.7544 - val_loss: 0.4774 - val_accuracy: 0.7522\n",
      "Epoch 29/100\n",
      "3126/3135 [============================>.] - ETA: 0s - loss: 0.4770 - accuracy: 0.7548\n",
      "Epoch 29: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4768 - accuracy: 0.7548 - val_loss: 0.4773 - val_accuracy: 0.7517\n",
      "Epoch 30/100\n",
      "3119/3135 [============================>.] - ETA: 0s - loss: 0.4767 - accuracy: 0.7549\n",
      "Epoch 30: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 9s 3ms/step - loss: 0.4767 - accuracy: 0.7548 - val_loss: 0.4772 - val_accuracy: 0.7517\n",
      "Epoch 31/100\n",
      "3121/3135 [============================>.] - ETA: 0s - loss: 0.4766 - accuracy: 0.7549\n",
      "Epoch 31: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 8s 3ms/step - loss: 0.4766 - accuracy: 0.7548 - val_loss: 0.4771 - val_accuracy: 0.7517\n",
      "Epoch 32/100\n",
      "3119/3135 [============================>.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7548\n",
      "Epoch 32: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4765 - accuracy: 0.7547 - val_loss: 0.4770 - val_accuracy: 0.7525\n",
      "Epoch 33/100\n",
      "3116/3135 [============================>.] - ETA: 0s - loss: 0.4764 - accuracy: 0.7548\n",
      "Epoch 33: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4764 - accuracy: 0.7546 - val_loss: 0.4769 - val_accuracy: 0.7525\n",
      "Epoch 34/100\n",
      "3113/3135 [============================>.] - ETA: 0s - loss: 0.4760 - accuracy: 0.7546\n",
      "Epoch 34: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 6s 2ms/step - loss: 0.4763 - accuracy: 0.7542 - val_loss: 0.4768 - val_accuracy: 0.7527\n",
      "Epoch 35/100\n",
      "3110/3135 [============================>.] - ETA: 0s - loss: 0.4759 - accuracy: 0.7547\n",
      "Epoch 35: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4762 - accuracy: 0.7542 - val_loss: 0.4767 - val_accuracy: 0.7527\n",
      "Epoch 36/100\n",
      "3125/3135 [============================>.] - ETA: 0s - loss: 0.4764 - accuracy: 0.7542\n",
      "Epoch 36: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 6s 2ms/step - loss: 0.4762 - accuracy: 0.7544 - val_loss: 0.4767 - val_accuracy: 0.7527\n",
      "Epoch 37/100\n",
      "3130/3135 [============================>.] - ETA: 0s - loss: 0.4762 - accuracy: 0.7542\n",
      "Epoch 37: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4761 - accuracy: 0.7542 - val_loss: 0.4766 - val_accuracy: 0.7527\n",
      "Epoch 38/100\n",
      "3129/3135 [============================>.] - ETA: 0s - loss: 0.4761 - accuracy: 0.7542\n",
      "Epoch 38: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4760 - accuracy: 0.7542 - val_loss: 0.4766 - val_accuracy: 0.7525\n",
      "Epoch 39/100\n",
      "3122/3135 [============================>.] - ETA: 0s - loss: 0.4760 - accuracy: 0.7542\n",
      "Epoch 39: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 6s 2ms/step - loss: 0.4759 - accuracy: 0.7542 - val_loss: 0.4765 - val_accuracy: 0.7525\n",
      "Epoch 40/100\n",
      "3117/3135 [============================>.] - ETA: 0s - loss: 0.4759 - accuracy: 0.7543\n",
      "Epoch 40: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4759 - accuracy: 0.7541 - val_loss: 0.4764 - val_accuracy: 0.7512\n",
      "Epoch 41/100\n",
      "3119/3135 [============================>.] - ETA: 0s - loss: 0.4758 - accuracy: 0.7542\n",
      "Epoch 41: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 6s 2ms/step - loss: 0.4758 - accuracy: 0.7541 - val_loss: 0.4763 - val_accuracy: 0.7512\n",
      "Epoch 42/100\n",
      "3118/3135 [============================>.] - ETA: 0s - loss: 0.4757 - accuracy: 0.7539\n",
      "Epoch 42: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 8s 2ms/step - loss: 0.4758 - accuracy: 0.7538 - val_loss: 0.4763 - val_accuracy: 0.7512\n",
      "Epoch 43/100\n",
      "3118/3135 [============================>.] - ETA: 0s - loss: 0.4757 - accuracy: 0.7541\n",
      "Epoch 43: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 10s 3ms/step - loss: 0.4757 - accuracy: 0.7540 - val_loss: 0.4762 - val_accuracy: 0.7525\n",
      "Epoch 44/100\n",
      "3121/3135 [============================>.] - ETA: 0s - loss: 0.4756 - accuracy: 0.7538\n",
      "Epoch 44: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 8s 3ms/step - loss: 0.4756 - accuracy: 0.7537 - val_loss: 0.4761 - val_accuracy: 0.7538\n",
      "Epoch 45/100\n",
      "3133/3135 [============================>.] - ETA: 0s - loss: 0.4758 - accuracy: 0.7538\n",
      "Epoch 45: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 8s 2ms/step - loss: 0.4756 - accuracy: 0.7539 - val_loss: 0.4761 - val_accuracy: 0.7525\n",
      "Epoch 46/100\n",
      "3129/3135 [============================>.] - ETA: 0s - loss: 0.4756 - accuracy: 0.7537\n",
      "Epoch 46: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 8s 2ms/step - loss: 0.4756 - accuracy: 0.7537 - val_loss: 0.4760 - val_accuracy: 0.7525\n",
      "Epoch 47/100\n",
      "3124/3135 [============================>.] - ETA: 0s - loss: 0.4758 - accuracy: 0.7538\n",
      "Epoch 47: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4755 - accuracy: 0.7539 - val_loss: 0.4762 - val_accuracy: 0.7522\n",
      "Epoch 48/100\n",
      "3117/3135 [============================>.] - ETA: 0s - loss: 0.4755 - accuracy: 0.7540\n",
      "Epoch 48: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4755 - accuracy: 0.7538 - val_loss: 0.4761 - val_accuracy: 0.7522\n",
      "Epoch 49/100\n",
      "3120/3135 [============================>.] - ETA: 0s - loss: 0.4755 - accuracy: 0.7540\n",
      "Epoch 49: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 9s 3ms/step - loss: 0.4754 - accuracy: 0.7539 - val_loss: 0.4760 - val_accuracy: 0.7522\n",
      "Epoch 50/100\n",
      "3123/3135 [============================>.] - ETA: 0s - loss: 0.4756 - accuracy: 0.7539\n",
      "Epoch 50: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4754 - accuracy: 0.7539 - val_loss: 0.4760 - val_accuracy: 0.7522\n",
      "Epoch 51/100\n",
      "3129/3135 [============================>.] - ETA: 0s - loss: 0.4754 - accuracy: 0.7540\n",
      "Epoch 51: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 9s 3ms/step - loss: 0.4753 - accuracy: 0.7539 - val_loss: 0.4759 - val_accuracy: 0.7522\n",
      "Epoch 52/100\n",
      "3122/3135 [============================>.] - ETA: 0s - loss: 0.4754 - accuracy: 0.7537\n",
      "Epoch 52: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 14s 5ms/step - loss: 0.4753 - accuracy: 0.7537 - val_loss: 0.4759 - val_accuracy: 0.7522\n",
      "Epoch 53/100\n",
      "3120/3135 [============================>.] - ETA: 0s - loss: 0.4753 - accuracy: 0.7538\n",
      "Epoch 53: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 16s 5ms/step - loss: 0.4753 - accuracy: 0.7537 - val_loss: 0.4759 - val_accuracy: 0.7522\n",
      "Epoch 54/100\n",
      "3133/3135 [============================>.] - ETA: 0s - loss: 0.4754 - accuracy: 0.7535\n",
      "Epoch 54: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 15s 5ms/step - loss: 0.4752 - accuracy: 0.7536 - val_loss: 0.4758 - val_accuracy: 0.7525\n",
      "Epoch 55/100\n",
      "3133/3135 [============================>.] - ETA: 0s - loss: 0.4754 - accuracy: 0.7534\n",
      "Epoch 55: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 12s 4ms/step - loss: 0.4752 - accuracy: 0.7535 - val_loss: 0.4758 - val_accuracy: 0.7525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "3129/3135 [============================>.] - ETA: 0s - loss: 0.4753 - accuracy: 0.7536\n",
      "Epoch 56: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 14s 4ms/step - loss: 0.4752 - accuracy: 0.7535 - val_loss: 0.4758 - val_accuracy: 0.7525\n",
      "Epoch 57/100\n",
      "3131/3135 [============================>.] - ETA: 0s - loss: 0.4754 - accuracy: 0.7533\n",
      "Epoch 57: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 14s 4ms/step - loss: 0.4751 - accuracy: 0.7534 - val_loss: 0.4757 - val_accuracy: 0.7525\n",
      "Epoch 58/100\n",
      "3130/3135 [============================>.] - ETA: 0s - loss: 0.4752 - accuracy: 0.7535\n",
      "Epoch 58: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 11s 4ms/step - loss: 0.4751 - accuracy: 0.7535 - val_loss: 0.4757 - val_accuracy: 0.7522\n",
      "Epoch 59/100\n",
      "3135/3135 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.7535\n",
      "Epoch 59: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 12s 4ms/step - loss: 0.4751 - accuracy: 0.7535 - val_loss: 0.4757 - val_accuracy: 0.7522\n",
      "Epoch 60/100\n",
      "3128/3135 [============================>.] - ETA: 0s - loss: 0.4751 - accuracy: 0.7534\n",
      "Epoch 60: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 11s 4ms/step - loss: 0.4751 - accuracy: 0.7534 - val_loss: 0.4757 - val_accuracy: 0.7522\n",
      "Epoch 61/100\n",
      "3135/3135 [==============================] - ETA: 0s - loss: 0.4750 - accuracy: 0.7535\n",
      "Epoch 61: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 10s 3ms/step - loss: 0.4750 - accuracy: 0.7535 - val_loss: 0.4757 - val_accuracy: 0.7520\n",
      "Epoch 62/100\n",
      "3120/3135 [============================>.] - ETA: 0s - loss: 0.4751 - accuracy: 0.7535\n",
      "Epoch 62: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 11s 3ms/step - loss: 0.4750 - accuracy: 0.7533 - val_loss: 0.4756 - val_accuracy: 0.7520\n",
      "Epoch 63/100\n",
      "3125/3135 [============================>.] - ETA: 0s - loss: 0.4753 - accuracy: 0.7532\n",
      "Epoch 63: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 9s 3ms/step - loss: 0.4750 - accuracy: 0.7533 - val_loss: 0.4756 - val_accuracy: 0.7520\n",
      "Epoch 64/100\n",
      "3116/3135 [============================>.] - ETA: 0s - loss: 0.4749 - accuracy: 0.7535\n",
      "Epoch 64: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 9s 3ms/step - loss: 0.4750 - accuracy: 0.7532 - val_loss: 0.4756 - val_accuracy: 0.7520\n",
      "Epoch 65/100\n",
      "3135/3135 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.7532\n",
      "Epoch 65: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 9s 3ms/step - loss: 0.4749 - accuracy: 0.7532 - val_loss: 0.4756 - val_accuracy: 0.7520\n",
      "Epoch 66/100\n",
      "3123/3135 [============================>.] - ETA: 0s - loss: 0.4751 - accuracy: 0.7533\n",
      "Epoch 66: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 10s 3ms/step - loss: 0.4749 - accuracy: 0.7533 - val_loss: 0.4756 - val_accuracy: 0.7520\n",
      "Epoch 67/100\n",
      "3126/3135 [============================>.] - ETA: 0s - loss: 0.4750 - accuracy: 0.7536\n",
      "Epoch 67: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 12s 4ms/step - loss: 0.4749 - accuracy: 0.7536 - val_loss: 0.4756 - val_accuracy: 0.7520\n",
      "Epoch 68/100\n",
      "3123/3135 [============================>.] - ETA: 0s - loss: 0.4750 - accuracy: 0.7533\n",
      "Epoch 68: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 12s 4ms/step - loss: 0.4749 - accuracy: 0.7533 - val_loss: 0.4755 - val_accuracy: 0.7520\n",
      "Epoch 69/100\n",
      "3133/3135 [============================>.] - ETA: 0s - loss: 0.4750 - accuracy: 0.7534\n",
      "Epoch 69: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 13s 4ms/step - loss: 0.4748 - accuracy: 0.7535 - val_loss: 0.4755 - val_accuracy: 0.7520\n",
      "Epoch 70/100\n",
      "3120/3135 [============================>.] - ETA: 0s - loss: 0.4749 - accuracy: 0.7534\n",
      "Epoch 70: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 11s 4ms/step - loss: 0.4748 - accuracy: 0.7533 - val_loss: 0.4754 - val_accuracy: 0.7522\n",
      "Epoch 71/100\n",
      "3130/3135 [============================>.] - ETA: 0s - loss: 0.4749 - accuracy: 0.7532\n",
      "Epoch 71: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 12s 4ms/step - loss: 0.4748 - accuracy: 0.7532 - val_loss: 0.4755 - val_accuracy: 0.7520\n",
      "Epoch 72/100\n",
      "3135/3135 [==============================] - ETA: 0s - loss: 0.4748 - accuracy: 0.7533\n",
      "Epoch 72: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 11s 3ms/step - loss: 0.4748 - accuracy: 0.7533 - val_loss: 0.4755 - val_accuracy: 0.7520\n",
      "Epoch 73/100\n",
      "3135/3135 [==============================] - ETA: 0s - loss: 0.4748 - accuracy: 0.7532\n",
      "Epoch 73: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 8s 3ms/step - loss: 0.4748 - accuracy: 0.7532 - val_loss: 0.4753 - val_accuracy: 0.7520\n",
      "Epoch 74/100\n",
      "3123/3135 [============================>.] - ETA: 0s - loss: 0.4749 - accuracy: 0.7532\n",
      "Epoch 74: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4747 - accuracy: 0.7532 - val_loss: 0.4753 - val_accuracy: 0.7520\n",
      "Epoch 75/100\n",
      "3111/3135 [============================>.] - ETA: 0s - loss: 0.4744 - accuracy: 0.7538\n",
      "Epoch 75: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4747 - accuracy: 0.7533 - val_loss: 0.4753 - val_accuracy: 0.7522\n",
      "Epoch 76/100\n",
      "3116/3135 [============================>.] - ETA: 0s - loss: 0.4746 - accuracy: 0.7533\n",
      "Epoch 76: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 6s 2ms/step - loss: 0.4747 - accuracy: 0.7531 - val_loss: 0.4753 - val_accuracy: 0.7522\n",
      "Epoch 77/100\n",
      "3110/3135 [============================>.] - ETA: 0s - loss: 0.4744 - accuracy: 0.7533\n",
      "Epoch 77: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4747 - accuracy: 0.7529 - val_loss: 0.4752 - val_accuracy: 0.7522\n",
      "Epoch 78/100\n",
      "3133/3135 [============================>.] - ETA: 0s - loss: 0.4748 - accuracy: 0.7530\n",
      "Epoch 78: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 6s 2ms/step - loss: 0.4747 - accuracy: 0.7530 - val_loss: 0.4752 - val_accuracy: 0.7522\n",
      "Epoch 79/100\n",
      "3132/3135 [============================>.] - ETA: 0s - loss: 0.4749 - accuracy: 0.7531\n",
      "Epoch 79: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4747 - accuracy: 0.7532 - val_loss: 0.4752 - val_accuracy: 0.7522\n",
      "Epoch 80/100\n",
      "3128/3135 [============================>.] - ETA: 0s - loss: 0.4747 - accuracy: 0.7531\n",
      "Epoch 80: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4746 - accuracy: 0.7532 - val_loss: 0.4752 - val_accuracy: 0.7535\n",
      "Epoch 81/100\n",
      "3135/3135 [==============================] - ETA: 0s - loss: 0.4746 - accuracy: 0.7530\n",
      "Epoch 81: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4746 - accuracy: 0.7530 - val_loss: 0.4751 - val_accuracy: 0.7535\n",
      "Epoch 82/100\n",
      "3129/3135 [============================>.] - ETA: 0s - loss: 0.4747 - accuracy: 0.7529\n",
      "Epoch 82: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 6s 2ms/step - loss: 0.4746 - accuracy: 0.7528 - val_loss: 0.4751 - val_accuracy: 0.7535\n",
      "Epoch 83/100\n",
      "3115/3135 [============================>.] - ETA: 0s - loss: 0.4745 - accuracy: 0.7530\n",
      "Epoch 83: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4746 - accuracy: 0.7527 - val_loss: 0.4751 - val_accuracy: 0.7535\n",
      "Epoch 84/100\n",
      "3131/3135 [============================>.] - ETA: 0s - loss: 0.4749 - accuracy: 0.7527\n",
      "Epoch 84: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4746 - accuracy: 0.7528 - val_loss: 0.4751 - val_accuracy: 0.7535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "3128/3135 [============================>.] - ETA: 0s - loss: 0.4747 - accuracy: 0.7526\n",
      "Epoch 85: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4746 - accuracy: 0.7526 - val_loss: 0.4751 - val_accuracy: 0.7535\n",
      "Epoch 86/100\n",
      "3114/3135 [============================>.] - ETA: 0s - loss: 0.4743 - accuracy: 0.7531\n",
      "Epoch 86: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 6s 2ms/step - loss: 0.4745 - accuracy: 0.7526 - val_loss: 0.4751 - val_accuracy: 0.7535\n",
      "Epoch 87/100\n",
      "3127/3135 [============================>.] - ETA: 0s - loss: 0.4746 - accuracy: 0.7525\n",
      "Epoch 87: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4745 - accuracy: 0.7525 - val_loss: 0.4751 - val_accuracy: 0.7535\n",
      "Epoch 88/100\n",
      "3132/3135 [============================>.] - ETA: 0s - loss: 0.4748 - accuracy: 0.7524\n",
      "Epoch 88: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4745 - accuracy: 0.7526 - val_loss: 0.4750 - val_accuracy: 0.7535\n",
      "Epoch 89/100\n",
      "3135/3135 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.7527\n",
      "Epoch 89: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4745 - accuracy: 0.7527 - val_loss: 0.4750 - val_accuracy: 0.7535\n",
      "Epoch 90/100\n",
      "3121/3135 [============================>.] - ETA: 0s - loss: 0.4744 - accuracy: 0.7530\n",
      "Epoch 90: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 6s 2ms/step - loss: 0.4745 - accuracy: 0.7529 - val_loss: 0.4750 - val_accuracy: 0.7535\n",
      "Epoch 91/100\n",
      "3121/3135 [============================>.] - ETA: 0s - loss: 0.4744 - accuracy: 0.7528\n",
      "Epoch 91: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 7s 2ms/step - loss: 0.4745 - accuracy: 0.7526 - val_loss: 0.4750 - val_accuracy: 0.7535\n",
      "Epoch 92/100\n",
      "3132/3135 [============================>.] - ETA: 0s - loss: 0.4748 - accuracy: 0.7527\n",
      "Epoch 92: val_accuracy did not improve from 0.75504\n",
      "3135/3135 [==============================] - 6s 2ms/step - loss: 0.4745 - accuracy: 0.7528 - val_loss: 0.4750 - val_accuracy: 0.7535\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 네트워크 구성\n",
    "# model.add(Dense(20, input_shape=(25, ), activation='relu'))\n",
    "model.add(Dense(5, input_dim=25, activation='relu'))\n",
    "# 0.98 0.01 0.01 0.98 0.01 0.01 0.98 0.01 0.01 -> 1 0 0 → 0 ~ 1 사이의 확률 5가지 출력, 총합은 1\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', \n",
    "                                metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "mcp = ModelCheckpoint(filepath='./3kki6.h5', monitor='val_accuracy',\n",
    "                      verbose=1, save_best_only=True)\n",
    "\n",
    "es = EarlyStopping(monitor='loss', patience=1, restore_best_weights=True)\n",
    "\n",
    "hist = model.fit(x_train, y_train, validation_data=(x_val, y_val), shuffle=False,\n",
    "                 epochs=100, batch_size=5, callbacks=[mcp, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "손실값: 0.4750111401081085 /정확도: 75.35085678100586 %\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "# plt.figure(figsize=(6,4)) # ERROR\n",
    "fig.set_size_inches(10, 5)  # 챠트 크기 설정\n",
    "\n",
    "acc_ax = loss_ax.twinx()  # 오른쪽 y 출 설정\n",
    "\n",
    "# 왼쪽 y 축 설정\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.5]) # 값을 반영하여 변경\n",
    "\n",
    "# 오른쪽 y 축 설정\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train accuracy')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val accuracy')\n",
    "acc_ax.set_ylim([0.0, 1.1])\n",
    "\n",
    "# 축 레이블 설정\n",
    "loss_ax.set_xlabel('epoch' )  # 학습 횟수\n",
    "loss_ax.set_ylabel('loss')    # 왼쪽 y 축 레이블, 오차\n",
    "acc_ax.set_ylabel('accuracy') # 오른쪽 y 축 레이블,정확도\n",
    "\n",
    "loss_ax.legend(loc='upper left') # 왼쪽 y 축 오차 레이블 위치\n",
    "acc_ax.legend(loc='lower left')  # 오른쪽 y 축 정확도 레이블 위치\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_val, y_val, batch_size=1, verbose=0)\n",
    "print('손실값:', test_loss, '/정확도:', (test_acc*100), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(path + '/Book.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터: (2178, 25)\n",
      "데이터: [0.01 0.01 0.01 0.96 0.01 0.96 0.01 0.01 0.01 0.01 0.01 0.01 0.96 0.01\n",
      " 0.01 0.96 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.96 0.01]\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "예측 결과 p.shape: (2178, 5)\n"
     ]
    }
   ],
   "source": [
    "print('데이터:', x_test.shape) # 변수가 9개로 구성된 4건의 관측치(행)\n",
    "print('데이터:', x_test[0])    # 첫번째 데이터행\n",
    "\n",
    "p = model.predict(x_test)      # 테스트 데이터 4건 ★\n",
    "print('예측 결과 p.shape:', p.shape)     # (4, 3): 3: 폼종의 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [6.3530654e-01 2.0167636e-06 1.7068901e-03 3.6298233e-01 2.2225649e-06]\n",
      "예측값의 합: 1.000\n",
      "예측값: 63.53065% 0.00020% 0.17069%\n",
      "One-hot-encoding:  [1. 0. 0. 0. 0.]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print('예측값:', p[0])        # 첫번째 예측값 출력, 확률 0 ~ 1사이의 실수값\n",
    "print('예측값의 합: {0:0.3f}'.format(np.sum(p[0])))\n",
    "print('예측값: {0:.5f}% {1:.5f}% {2:.5f}%'.format(p[0,0]*100,p[0,1]*100,p[0,2]*100))\n",
    "print('One-hot-encoding: ', y_test[0])\n",
    "print(np.argmax(p[0]))      # 가장 큰값의 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3530654e-01 2.0167636e-06 1.7068901e-03 3.6298233e-01 2.2225649e-06]\n",
      " [1.7017986e-04 1.2449580e-03 5.6994694e-01 4.2863405e-01 3.7900606e-06]\n",
      " [1.1538106e-03 2.9497873e-04 1.5026773e-03 9.9704748e-01 1.1226291e-06]\n",
      " ...\n",
      " [2.3019653e-07 1.2798951e-10 7.9607793e-07 1.7830736e-13 9.9999893e-01]\n",
      " [2.1715101e-03 8.5983239e-04 4.5294883e-06 9.9577504e-01 1.1891844e-03]\n",
      " [5.4160374e-01 4.5815694e-01 9.3220501e-07 2.3588669e-04 2.5417989e-06]]\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
